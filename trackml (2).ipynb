{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7878,"databundleVersionId":46689,"sourceType":"competition"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:16:40.971812Z","iopub.execute_input":"2026-01-04T12:16:40.972464Z","iopub.status.idle":"2026-01-04T12:16:41.254907Z","shell.execute_reply.started":"2026-01-04T12:16:40.972422Z","shell.execute_reply":"2026-01-04T12:16:41.254327Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/trackml-particle-identification/train_sample.zip\n/kaggle/input/trackml-particle-identification/train_3.zip\n/kaggle/input/trackml-particle-identification/train_2.zip\n/kaggle/input/trackml-particle-identification/sample_submission.csv.zip\n/kaggle/input/trackml-particle-identification/train_5.zip\n/kaggle/input/trackml-particle-identification/detectors.zip\n/kaggle/input/trackml-particle-identification/test.zip\n/kaggle/input/trackml-particle-identification/blacklist_training.zip\n/kaggle/input/trackml-particle-identification/train_4.zip\n/kaggle/input/trackml-particle-identification/train_1.zip\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torch_geometric torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-2.8.0+cu126.html\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:16:46.767830Z","iopub.execute_input":"2026-01-04T12:16:46.768511Z","iopub.status.idle":"2026-01-04T12:16:53.842953Z","shell.execute_reply.started":"2026-01-04T12:16:46.768482Z","shell.execute_reply":"2026-01-04T12:16:53.842255Z"}},"outputs":[{"name":"stdout","text":"Looking in links: https://data.pyg.org/whl/torch-2.8.0+cu126.html\nCollecting torch_geometric\n  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting torch-scatter\n  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcu126/torch_scatter-2.1.2%2Bpt28cu126-cp312-cp312-linux_x86_64.whl (10.9 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting torch-sparse\n  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcu126/torch_sparse-0.6.18%2Bpt28cu126-cp312-cp312-linux_x86_64.whl (5.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torch-cluster\n  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcu126/torch_cluster-1.6.3%2Bpt28cu126-cp312-cp312-linux_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.10.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.15.3)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.11.12)\nRequirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\nDownloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-scatter, torch-sparse, torch-cluster, torch_geometric\nSuccessfully installed torch-cluster-1.6.3+pt28cu126 torch-scatter-2.1.2+pt28cu126 torch-sparse-0.6.18+pt28cu126 torch_geometric-2.7.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch_geometric.data import Data, Dataset\nfrom torch_geometric.transforms import KNNGraph\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import TransformerConv\nimport numpy as np\nimport os\nimport pandas as pd\nimport pytorch_lightning as pl\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport zipfile\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom collections import Counter\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:16:59.913305Z","iopub.execute_input":"2026-01-04T12:16:59.913617Z","iopub.status.idle":"2026-01-04T12:17:16.094063Z","shell.execute_reply.started":"2026-01-04T12:16:59.913587Z","shell.execute_reply":"2026-01-04T12:17:16.093450Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()\nprint(\" GPU memory cleared\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:17:19.174881Z","iopub.execute_input":"2026-01-04T12:17:19.175664Z","iopub.status.idle":"2026-01-04T12:17:19.404364Z","shell.execute_reply.started":"2026-01-04T12:17:19.175626Z","shell.execute_reply":"2026-01-04T12:17:19.403415Z"}},"outputs":[{"name":"stdout","text":" GPU memory cleared\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\n# Check available GPU memory\nif torch.cuda.is_available():\n    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"Total GPU memory: {total_memory:.2f} GB\")\n    print(f\"Available GPU memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB allocated\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:17:22.952915Z","iopub.execute_input":"2026-01-04T12:17:22.953640Z","iopub.status.idle":"2026-01-04T12:17:22.988835Z","shell.execute_reply.started":"2026-01-04T12:17:22.953606Z","shell.execute_reply":"2026-01-04T12:17:22.987924Z"}},"outputs":[{"name":"stdout","text":"Total GPU memory: 14.74 GB\nAvailable GPU memory: 0.00 GB allocated\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Data exploration utility\ndef explore_data_files(root):\n    \"\"\"Explore the structure of data files\"\"\"\n\n    print(\"DATA FILE EXPLORATION\")\n\n    \n    data_files = {\n        'train_sample': 'train_sample.zip',\n        'train_1': 'train_1.zip',\n        'train_2': 'train_2.zip',\n        'train_3': 'train_3.zip',\n        'train_4': 'train_4.zip',\n        'train_5': 'train_5.zip',\n        'test': 'test.zip',\n    }\n    \n    for name, filename in data_files.items():\n        filepath = os.path.join(root, filename)\n        if os.path.exists(filepath):\n            with zipfile.ZipFile(filepath, 'r') as z:\n                files = z.namelist()\n                events = len([f for f in files if f.endswith('-hits.csv')])\n                print(f\"\\n{name:15} ({filename:20}): {events:4} events\")\n        else:\n            print(f\"\\n{name:15} ({filename:20}): NOT FOUND\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:17:25.921324Z","iopub.execute_input":"2026-01-04T12:17:25.921854Z","iopub.status.idle":"2026-01-04T12:17:25.927718Z","shell.execute_reply.started":"2026-01-04T12:17:25.921818Z","shell.execute_reply":"2026-01-04T12:17:25.926905Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Explore data before training\nroot = '/kaggle/input/trackml-particle-identification'\nexplore_data_files(root)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:17:29.417835Z","iopub.execute_input":"2026-01-04T12:17:29.418158Z","iopub.status.idle":"2026-01-04T12:17:29.747802Z","shell.execute_reply.started":"2026-01-04T12:17:29.418124Z","shell.execute_reply":"2026-01-04T12:17:29.747063Z"}},"outputs":[{"name":"stdout","text":"DATA FILE EXPLORATION\n\ntrain_sample    (train_sample.zip    ):  100 events\n\ntrain_1         (train_1.zip         ): 1770 events\n\ntrain_2         (train_2.zip         ): 1770 events\n\ntrain_3         (train_3.zip         ): 1770 events\n\ntrain_4         (train_4.zip         ): 1770 events\n\ntrain_5         (train_5.zip         ): 1770 events\n\ntest            (test.zip            ):  125 events\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Manual data loading from zip\ndef load_event(zip_path, inner_path):\n    with zipfile.ZipFile(zip_path, 'r') as z:\n        hits = pd.read_csv(z.open(f'{inner_path}-hits.csv'))\n        cells = pd.read_csv(z.open(f'{inner_path}-cells.csv'))\n        particles = pd.read_csv(z.open(f'{inner_path}-particles.csv'))\n        truth = pd.read_csv(z.open(f'{inner_path}-truth.csv'))\n    return hits, cells, particles, truth\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:17:32.716917Z","iopub.execute_input":"2026-01-04T12:17:32.717247Z","iopub.status.idle":"2026-01-04T12:17:32.721924Z","shell.execute_reply.started":"2026-01-04T12:17:32.717215Z","shell.execute_reply":"2026-01-04T12:17:32.721238Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Dataset class adjusted for Kaggle zip structure with memory-efficient sampling\nclass TrackMLDataset(Dataset):\n    def __init__(self, root, zip_name='train_sample.zip', n_events=8, transform=None, max_hits=30000):\n        super().__init__(root, transform=transform)\n        self.zip_path = os.path.join(root, zip_name)\n        self.max_hits = max_hits  # Configurable hit limit\n        # Reduce k from 8 to 6 to save memory\n        self.transform = transform or KNNGraph(k=6)\n        \n        # Dynamically extract unique event paths from zip namelist\n        self.inner_paths = []\n        with zipfile.ZipFile(self.zip_path, 'r') as z:\n            for fname in z.namelist():\n                if fname.endswith('-hits.csv'):\n                    inner = fname.rsplit('-hits.csv', 1)[0]\n                    self.inner_paths.append(inner)\n        self.inner_paths = sorted(set(self.inner_paths))[:n_events]\n\n    def len(self):\n        return len(self.inner_paths)\n\n    def get(self, idx):\n        inner_path = self.inner_paths[idx]\n        hits, cells, particles, truth = load_event(self.zip_path, inner_path)\n        \n        # MEMORY OPTIMIZATION: Sample hits if too many\n        if len(hits) > self.max_hits:\n            sample_idx = np.random.choice(len(hits), self.max_hits, replace=False)\n            sample_idx = np.sort(sample_idx)  # Keep order for consistency\n            hits = hits.iloc[sample_idx].reset_index(drop=True)\n            # Update truth to match sampled hits\n            truth = truth[truth['hit_id'].isin(hits['hit_id'])].reset_index(drop=True)\n        \n        # Node features (r, phi, z)\n        r = np.sqrt(hits['x']**2 + hits['y']**2).values\n        phi = np.arctan2(hits['y'], hits['x']).values\n        z = hits['z'].values\n        x = torch.tensor(np.stack([r, phi, z], axis=1), dtype=torch.float)\n        \n        # Store global coordinates for KNN calculation\n        pos = torch.tensor(hits[['x', 'y', 'z']].values, dtype=torch.float)\n        \n        data = Data(x=x, pos=pos)\n        \n        # Apply KNN Transform to create edges\n        if self.transform:\n            data = self.transform(data)\n            \n            # Label the edges: 1 if both hits belong to the same particle, else 0\n            hit_to_particle = truth.set_index('hit_id')['particle_id'].to_dict()\n            \n            raw_hit_ids = hits['hit_id'].values\n            edge_src_pids = np.array([hit_to_particle.get(raw_hit_ids[i], 0) for i in data.edge_index[0]])\n            edge_tgt_pids = np.array([hit_to_particle.get(raw_hit_ids[i], 0) for i in data.edge_index[1]])\n            \n            # Logic: same particle AND not noise (PID 0 is noise)\n            y = (edge_src_pids == edge_tgt_pids) & (edge_src_pids != 0)\n            data.y = torch.tensor(y, dtype=torch.float)\n            \n        return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:17:36.795421Z","iopub.execute_input":"2026-01-04T12:17:36.796075Z","iopub.status.idle":"2026-01-04T12:17:36.806358Z","shell.execute_reply.started":"2026-01-04T12:17:36.796039Z","shell.execute_reply":"2026-01-04T12:17:36.805517Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass FocalLoss(torch.nn.Module):\n    def __init__(self, alpha=0.45, gamma=2.0):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets):\n        # Use built-in stable BCE\n        p = torch.sigmoid(inputs)\n        # Add epsilon (1e-7) to prevent log(0) and numerical underflow in 16-bit\n        ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n        p_t = p * targets + (1 - p) * (1 - targets)\n        loss = ce_loss * ((1 - p_t) ** self.gamma)\n\n        if self.alpha >= 0:\n            alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n            loss = alpha_t * loss\n\n        return loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:17:40.531882Z","iopub.execute_input":"2026-01-04T12:17:40.532431Z","iopub.status.idle":"2026-01-04T12:17:40.538241Z","shell.execute_reply.started":"2026-01-04T12:17:40.532394Z","shell.execute_reply":"2026-01-04T12:17:40.537540Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class TrackLightning(pl.LightningModule):\n    def __init__(self, model, pos_threshold=0.35):\n        super().__init__()\n        self.model = model\n        self.pos_threshold = pos_threshold\n        \n        self.loss_fn = FocalLoss(alpha=0.45, gamma=2.0)\n        \n        self.validation_step_outputs = []\n\n    def forward(self, batch):\n        return self.model(batch.x, batch.edge_index, batch.batch)\n\n    def training_step(self, batch, batch_idx):\n        logits = self(batch)\n        logits = torch.clamp(logits, min=-10, max=10) # Numerical stability\n        \n        loss = self.loss_fn(logits, batch.y.float())\n        \n        if torch.isnan(loss):\n            return None # Skip bad batches\n            \n        with torch.no_grad():\n            # Lower threshold helps catch more tracks in early training\n            preds = torch.sigmoid(logits) > self.pos_threshold\n            acc = (preds == batch.y).float().mean()\n        \n        self.log('train_loss', loss, prog_bar=True, batch_size=batch.num_graphs)\n        self.log('train_acc', acc, prog_bar=True, batch_size=batch.num_graphs)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        logits = self(batch)\n        loss = self.loss_fn(logits, batch.y.float())\n        \n        # Lower threshold for validation too\n        preds = (torch.sigmoid(logits) > self.pos_threshold).cpu()\n        targets = batch.y.cpu()\n        \n        self.validation_step_outputs.append({'preds': preds, 'targets': targets})\n        \n        acc = (preds == targets).float().mean()\n        self.log('val_loss', loss, prog_bar=True, batch_size=batch.num_graphs)\n        self.log('val_acc', acc, prog_bar=True, batch_size=batch.num_graphs)\n        return loss\n\n    def on_validation_epoch_end(self):\n        if not self.validation_step_outputs:\n            return\n            \n        all_preds = torch.cat([x['preds'] for x in self.validation_step_outputs]).numpy()\n        all_targets = torch.cat([x['targets'] for x in self.validation_step_outputs]).numpy()\n        \n        accuracy = accuracy_score(all_targets, all_preds)\n        precision = precision_score(all_targets, all_preds, zero_division=0)\n        recall = recall_score(all_targets, all_preds, zero_division=0)\n        f1 = f1_score(all_targets, all_preds, zero_division=0)\n        \n        # IMPORTANT: Log these for the progress bar\n        self.log('val_f1', f1, prog_bar=True)\n        self.log('val_recall', recall, prog_bar=True)\n        self.log('val_precision', precision)\n        \n        self.validation_step_outputs.clear()\n        torch.cuda.empty_cache()\n\n    def configure_optimizers(self):\n        # Slower learning rate for Focal Loss stability\n        optimizer = AdamW(self.parameters(), lr=3e-4, weight_decay=1e-5)\n        \n        # CRITICAL: Monitor val_f1 instead of val_loss\n        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n        return {\n            'optimizer': optimizer, \n            'lr_scheduler': {\n                'scheduler': scheduler, \n                'monitor': 'val_f1' \n            }\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:17:44.141186Z","iopub.execute_input":"2026-01-04T12:17:44.142031Z","iopub.status.idle":"2026-01-04T12:17:44.154433Z","shell.execute_reply.started":"2026-01-04T12:17:44.141958Z","shell.execute_reply":"2026-01-04T12:17:44.153635Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class TrackTransformer(torch.nn.Module):\n    def __init__(self, in_channels=3, hidden_channels=32, out_channels=1, heads=4):\n        super().__init__()\n        self.hidden_channels = hidden_channels\n        self.heads = heads\n        \n        self.embed = torch.nn.Linear(in_channels, hidden_channels)\n        # FIX 7: Add BatchNorm for stability\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        \n        self.conv1 = TransformerConv(hidden_channels, hidden_channels, heads=heads, \n                                     dropout=0.1, edge_dim=hidden_channels, beta=True)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels * heads)\n        \n        self.conv2 = TransformerConv(hidden_channels * heads, hidden_channels, heads=heads, \n                                     dropout=0.1, edge_dim=hidden_channels * heads, beta=True)\n        self.bn3 = torch.nn.BatchNorm1d(hidden_channels * heads)\n        \n        self.conv3 = TransformerConv(hidden_channels * heads, hidden_channels // heads, heads=heads, \n                                     dropout=0.1, edge_dim=hidden_channels * heads, beta=True)\n        \n        self.lin_edge = torch.nn.Linear((hidden_channels // heads) * heads * 2, out_channels)\n\n    def forward(self, x, edge_index, batch=None):\n        x = self.embed(x)\n        x = self.bn1(x)  # BatchNorm after embedding\n        x = F.relu(x)\n        \n        # Layer 1\n        src, tgt = x[edge_index[0]], x[edge_index[1]]\n        edge_attr = torch.abs(src - tgt)\n        x = self.conv1(x, edge_index, edge_attr)\n        x = self.bn2(x)  # BatchNorm after conv\n        x = F.relu(x)\n        \n        # Layer 2\n        src, tgt = x[edge_index[0]], x[edge_index[1]]\n        edge_attr = torch.abs(src - tgt)\n        x = self.conv2(x, edge_index, edge_attr)\n        x = self.bn3(x)  # BatchNorm after conv\n        x = F.relu(x)\n        \n        # Layer 3\n        src, tgt = x[edge_index[0]], x[edge_index[1]]\n        edge_attr = torch.abs(src - tgt)\n        x = self.conv3(x, edge_index, edge_attr)\n        \n        # Edge classification\n        edge_emb = torch.cat([x[edge_index[0]], x[edge_index[1]]], dim=-1)\n        return self.lin_edge(edge_emb).squeeze(-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:17:48.035410Z","iopub.execute_input":"2026-01-04T12:17:48.035915Z","iopub.status.idle":"2026-01-04T12:17:48.044593Z","shell.execute_reply.started":"2026-01-04T12:17:48.035881Z","shell.execute_reply":"2026-01-04T12:17:48.043960Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Prepare data with train/val split using multiple training files\nroot = '/kaggle/input/trackml-particle-identification'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:17:54.733969Z","iopub.execute_input":"2026-01-04T12:17:54.734636Z","iopub.status.idle":"2026-01-04T12:17:54.737993Z","shell.execute_reply.started":"2026-01-04T12:17:54.734599Z","shell.execute_reply":"2026-01-04T12:17:54.737220Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Load training data with more events (can handle more with batching!)\nprint(\"\\nLoading training data...\")\ntrain_dataset = TrackMLDataset(root, zip_name='train_sample.zip', n_events=30)  # More events with batching!\nprint(f\"  Training events: {len(train_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:17:56.753910Z","iopub.execute_input":"2026-01-04T12:17:56.754426Z","iopub.status.idle":"2026-01-04T12:17:56.763299Z","shell.execute_reply.started":"2026-01-04T12:17:56.754389Z","shell.execute_reply":"2026-01-04T12:17:56.762608Z"}},"outputs":[{"name":"stdout","text":"\nLoading training data...\n  Training events: 30\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Load validation data\nprint(\"Loading validation data...\")\nval_dataset = TrackMLDataset(root, zip_name='train_sample.zip', n_events=10)\nprint(f\"  Validation events: {len(val_dataset)}\")\n\nfull_train_dataset = train_dataset\n\nprint(f\"Dataset Summary:\")\nprint(f\"  Training:   {len(full_train_dataset)} events\")\nprint(f\"  Validation: {len(val_dataset)} events\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:18:00.236903Z","iopub.execute_input":"2026-01-04T12:18:00.237245Z","iopub.status.idle":"2026-01-04T12:18:00.245285Z","shell.execute_reply.started":"2026-01-04T12:18:00.237212Z","shell.execute_reply":"2026-01-04T12:18:00.244524Z"}},"outputs":[{"name":"stdout","text":"Loading validation data...\n  Validation events: 10\nDataset Summary:\n  Training:   30 events\n  Validation: 10 events\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"BATCH_SIZE = 2  # Process 2 events at once\n\ntrain_loader = DataLoader(\n    full_train_dataset, \n    batch_size=BATCH_SIZE,  # Batch multiple graphs together!\n    shuffle=True, \n    num_workers=3,\n    pin_memory=True  # Faster GPU transfer\n)\n\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=BATCH_SIZE,  # Same batch size for validation\n    shuffle=False, \n    num_workers=3,\n    pin_memory=True\n)\n\nprint(f\" DataLoaders created:\")\nprint(f\"   Training batches: {len(train_loader)} (batch_size={BATCH_SIZE})\")\nprint(f\"   Validation batches: {len(val_loader)} (batch_size={BATCH_SIZE})\")\nprint(f\"   Total training graphs: {len(full_train_dataset)}\")\nprint(f\"   Total validation graphs: {len(val_dataset)}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:18:04.333374Z","iopub.execute_input":"2026-01-04T12:18:04.334081Z","iopub.status.idle":"2026-01-04T12:18:04.339793Z","shell.execute_reply.started":"2026-01-04T12:18:04.334045Z","shell.execute_reply":"2026-01-04T12:18:04.339115Z"}},"outputs":[{"name":"stdout","text":" DataLoaders created:\n   Training batches: 15 (batch_size=2)\n   Validation batches: 5 (batch_size=2)\n   Total training graphs: 30\n   Total validation graphs: 10\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Train\nmodel = TrackTransformer()\nlightning_model = TrackLightning(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:18:07.916132Z","iopub.execute_input":"2026-01-04T12:18:07.916515Z","iopub.status.idle":"2026-01-04T12:18:07.967548Z","shell.execute_reply.started":"2026-01-04T12:18:07.916483Z","shell.execute_reply":"2026-01-04T12:18:07.966961Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import os\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\n# 1. Create directory safely\nos.makedirs('checkpoints', exist_ok=True)\n\n# 2. Re-define callbacks with clear monitoring\ncheckpoint_callback = ModelCheckpoint(\n    monitor='val_f1',\n    dirpath='checkpoints',\n    filename='track-best-{epoch:02d}-{val_f1:.3f}',\n    save_top_k=1,\n    mode='max'\n)\n\nearly_stop_callback = EarlyStopping(\n    monitor='val_f1',\n    patience=10,\n    mode='max',\n    verbose=True\n)\n\n# 3. Trainer with safety settings\ntrainer = pl.Trainer(\n    max_epochs=25,\n    accelerator='gpu',\n    devices=1,\n    precision='16-mixed',\n    gradient_clip_val=0.5,\n    callbacks=[checkpoint_callback, early_stop_callback],\n    # If it still crashes, change this to 0 to skip the \"practice\" run\n    num_sanity_val_steps=2 \n)\n\nprint(\"ğŸš€ Starting Training...\")\ntrainer.fit(lightning_model, train_loader, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:18:10.453929Z","iopub.execute_input":"2026-01-04T12:18:10.454662Z","iopub.status.idle":"2026-01-04T12:24:47.768799Z","shell.execute_reply.started":"2026-01-04T12:18:10.454626Z","shell.execute_reply":"2026-01-04T12:24:47.767977Z"}},"outputs":[{"name":"stderr","text":"Using 16bit Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\n","output_type":"stream"},{"name":"stdout","text":"ğŸš€ Starting Training...\n","output_type":"stream"},{"name":"stderr","text":"2026-01-04 12:18:12.740625: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767529092.955430      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767529093.016287      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767529093.525897      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767529093.525934      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767529093.525938      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767529093.525942      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n/usr/local/lib/python3.12/dist-packages/pytorch_lightning/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\nâ”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\nâ”‚\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0mâ”‚ model   â”‚ TrackTransformer â”‚  125 K â”‚ train â”‚     0 â”‚\nâ”‚\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0mâ”‚ loss_fn â”‚ FocalLoss        â”‚      0 â”‚ train â”‚     0 â”‚\nâ””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>â”ƒ\nâ”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\nâ”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>â”‚ model   â”‚ TrackTransformer â”‚  125 K â”‚ train â”‚     0 â”‚\nâ”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>â”‚ loss_fn â”‚ FocalLoss        â”‚      0 â”‚ train â”‚     0 â”‚\nâ””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mTrainable params\u001b[0m: 125 K                                                                                            \n\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n\u001b[1mTotal params\u001b[0m: 125 K                                                                                                \n\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n\u001b[1mModules in train mode\u001b[0m: 31                                                                                          \n\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 125 K                                                                                            \n<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n<span style=\"font-weight: bold\">Total params</span>: 125 K                                                                                                \n<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n<span style=\"font-weight: bold\">Modules in train mode</span>: 31                                                                                          \n<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fa6fdd37f50414eba6d517aab75c6c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/usr/local/lib/python3.12/dist-packages/pytorch_lightning/loops/fit_loop.py:317: The number of training batches \n(15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if\nyou want to see logs for the training epoch.\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.12/dist-packages/pytorch_lightning/loops/fit_loop.py:317: The number of training batches \n(15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if\nyou want to see logs for the training epoch.\n</pre>\n"},"metadata":{}},{"name":"stderr","text":"Metric val_f1 improved. New best score: 0.039\nMetric val_f1 improved by 0.003 >= min_delta = 0.0. New best score: 0.043\nMonitored metric val_f1 did not improve in the last 10 records. Best score: 0.043. Signaling Trainer to stop.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# 1. Initialize and Load (Same as before)\nbase_model = TrackTransformer(in_channels=3, hidden_channels=32, out_channels=1, heads=2)\nmodel_eval = TrackLightning.load_from_checkpoint(\n    'checkpoints/trackml-epoch=04-val_f1=0.0700.ckpt', \n    model=base_model\n)\n\n# 2. Use validate() instead of test() to avoid the error\ntrainer.validate(model_eval, dataloaders=val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T06:57:24.693285Z","iopub.execute_input":"2026-01-04T06:57:24.694018Z","iopub.status.idle":"2026-01-04T06:57:30.125461Z","shell.execute_reply.started":"2026-01-04T06:57:24.693983Z","shell.execute_reply":"2026-01-04T06:57:30.124847Z"}},"outputs":[{"name":"stderr","text":"LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca44cd53a6e44ce395d49283162a21cf"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚\u001b[36m \u001b[0m\u001b[36m         val_acc         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.8679221868515015    \u001b[0m\u001b[35m \u001b[0mâ”‚\nâ”‚\u001b[36m \u001b[0m\u001b[36m      val_accuracy       \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.8679222464561462    \u001b[0m\u001b[35m \u001b[0mâ”‚\nâ”‚\u001b[36m \u001b[0m\u001b[36m         val_f1          \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.06688123196363449   \u001b[0m\u001b[35m \u001b[0mâ”‚\nâ”‚\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m           nan           \u001b[0m\u001b[35m \u001b[0mâ”‚\nâ”‚\u001b[36m \u001b[0m\u001b[36m      val_precision      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.03919358551502228   \u001b[0m\u001b[35m \u001b[0mâ”‚\nâ”‚\u001b[36m \u001b[0m\u001b[36m       val_recall        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.22782272100448608   \u001b[0m\u001b[35m \u001b[0mâ”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\">      Validate metric      </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚<span style=\"color: #008080; text-decoration-color: #008080\">          val_acc          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.8679221868515015     </span>â”‚\nâ”‚<span style=\"color: #008080; text-decoration-color: #008080\">       val_accuracy        </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.8679222464561462     </span>â”‚\nâ”‚<span style=\"color: #008080; text-decoration-color: #008080\">          val_f1           </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.06688123196363449    </span>â”‚\nâ”‚<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">            nan            </span>â”‚\nâ”‚<span style=\"color: #008080; text-decoration-color: #008080\">       val_precision       </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.03919358551502228    </span>â”‚\nâ”‚<span style=\"color: #008080; text-decoration-color: #008080\">        val_recall         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.22782272100448608    </span>â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"[{'val_loss': nan,\n  'val_acc': 0.8679221868515015,\n  'val_accuracy': 0.8679222464561462,\n  'val_precision': 0.03919358551502228,\n  'val_recall': 0.22782272100448608,\n  'val_f1': 0.06688123196363449}]"},"metadata":{}}],"execution_count":21}]}